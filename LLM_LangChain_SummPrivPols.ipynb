{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michellebonat/LLM-LangChain-PP-Summ/blob/main/LLM_LangChain_SummPrivPols.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMs and LangChain: Summarize a long privacy policy with various approaches\n",
        "\n",
        "**Purpose** This notebook records an experiment to understand if company privacy policies can be understood more easily by using the summarization features in LLMs and LangChain.\n",
        "\n",
        "Company privacy policies are typically very long and written in legal jargon. As a result, it's difficult to understand as a consumer what you may be agreeing to when you agree to their terms. Let's see if we can make this easier.\n",
        "\n",
        "**Why is this important?** The example company privacy policy we use here is TikTok. We chose this because of it's recent attention. The US government is trying to ban TikTok becuase of it's allegedly offensive ways that is uses consumer data. 170 million people in the US use TikTok and there are fears that it's owhers the Chinese government are spying on Americans through the use of this app and the capturing of their data.\n",
        "\n",
        "As of April 2024, \"the Senate passed legislation that would force TikTok's China-based parent company to sell the social media platform under the threat of a ban. A measure to outlaw the popular video-sharing app has won congressional approval and is on its way to President Biden for his signature. The measure gives Beijing-based parent company ByteDance nine months to sell the company, with a possible additional three months if a sale is in progress. If it doesn’t, TikTok will be banned.\" (Source: AP News) [What a TikTok ban in the US could mean for you.](https://apnews.com/article/tiktok-divestment-ban-what-you-need-to-know-5e1ff786e89da10a1b799241ae025406)\n",
        "\n",
        "**How can this project help?** Since company privacy policies are long and confusing, perhaps US consumers did not read what they were agreeing to by using TikTok. If the information was shorter and more simple perhaps they would be more informed about the data policies and perhaps they would make different choices. They may still use it, but perhaps they would look into changing the app data settings or be more careful what they post.\n",
        "\n",
        "References:\n",
        "* The Anthropic API information and signup can be found [here.](https://www.anthropic.com/)\n",
        "* The OpenAI API information and signup can be found [here.](https://platform.openai.com/)\n",
        "* This notebook was adapted from Roger Oriol's blog post about summarizing long documents with LLMs which can be found [here.](https://www.ruxu.dev/articles/ai/summarize-long-docs/)\n",
        "\n",
        "Version notes:\n",
        "* For these examples we will be using LangChain and Anthropic's Claude Sonnet model via their api which we set to Claude-3-sonnet-20240229 as of May 2024. This is the medium sized option, using the Evaluation plan tier (non-production).\n",
        "\n",
        "* For OpenAI we are using TODO (INSERT VERSION INFO HERE WHEN ADDED)\n"
      ],
      "metadata": {
        "id": "_nz-Pmp1asuA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "8AOzaXqTwRyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install the required Anthropic LangChain**"
      ],
      "metadata": {
        "id": "7uoyygWBglzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain langchain-anthropic langchain-text-splitters"
      ],
      "metadata": {
        "id": "pL_MkzGQ9PfQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96f99cbb-0807-4b4c-fa9f-eac76a3ef3b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.6/120.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m870.8/870.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup to use the model**\n",
        "\n",
        "❗ Remember to include your own API key for Anthropic in the api_key param below. Otherwise all following cells will fail. If you are doing this in a colab notebook, you can easily setup your own secrets. Find this by clicking on the left side navigation on the key icon."
      ],
      "metadata": {
        "id": "ML4AaY60eMSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_anthropic import ChatAnthropic\n",
        "from google.colab import userdata\n",
        "\n",
        "model = ChatAnthropic(\n",
        "    temperature=0,\n",
        "    api_key=userdata.get('michelle-secret-key-anthropic'),\n",
        "    model_name='claude-3-sonnet-20240229')"
      ],
      "metadata": {
        "id": "Gf681EhM-hAE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Two Approaches"
      ],
      "metadata": {
        "id": "yyJ1OsYmzTl2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarize short text snippet directly with direct imput\n",
        "\n",
        "Let's try a simple approach and see how useful it might be. To summarize a short bit of text, we can just copy it from the internet where the privacy policy is posted, then paste the text in the prompt in this notebook and ask the model to summarize it. We aren't using LangChain's stuff documents chain yet. Below we have manually pasted in the top part of the TikTok privacy policy."
      ],
      "metadata": {
        "id": "-3dXRjKPzcG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Write a concise summary of the following text:\n",
        "This Privacy Policy applies to TikTok services (the “Platform”), which include TikTok apps, websites, software\n",
        "and related services accessed via any platform or device that link to this Privacy Policy. The Platform is provided\n",
        " and controlled by TikTok Inc. (“TikTok”, “we” or “us”). We are committed to protecting and respecting your privacy.\n",
        "  This Privacy Policy explains how we collect, use, share, and otherwise process the personal information of users\n",
        "  and other individuals age 13 and over in connection with our Platform. For information about how we collect, use,\n",
        "   share, and otherwise process the personal information of users under age 13 (“Children”), please refer to our\n",
        "   Children’s Privacy Policy. For information about how we collect, use, share, and otherwise process consumer\n",
        "   health data as defined under Washington’s My Health My Data Act and other similar state laws, please refer\n",
        "   to the Consumer Health Data Privacy Policy.\n",
        "\n",
        "Summary: \"\"\""
      ],
      "metadata": {
        "id": "yfs4-7gw_aQM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(prompt)\n",
        "print(response.content)\n",
        "total_tokens = response.response_metadata['usage']['input_tokens'] + response.response_metadata['usage']['output_tokens']\n",
        "print(f\"Tokens used: {total_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru9kQApN_yOK",
        "outputId": "6c0562b4-06f8-402d-96c3-ab9ebb409bc5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is TikTok's Privacy Policy, which outlines how the company collects, uses, shares, and processes personal information of users aged 13 and over who access TikTok's services, including apps, websites, software, and related services. It also mentions separate policies for handling data of children under 13 and consumer health data, which are covered in the Children's Privacy Policy and Consumer Health Data Privacy Policy, respectively.\n",
            "Tokens used: 330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:** This is much shorter. However it is so short as to not be very useful. It also by default responds in a single line of text that is difficult to read (you need to scroll). Let's continue to discover some other approaches."
      ],
      "metadata": {
        "id": "_73jAQrvkWht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarize the text using MapReduce and LangChain\n",
        "\n",
        "This time we will use a longer block of text using the entire TikTok privacy policy. We will get it directly from it's source location online using LangChain's BeautifulSoup plugin."
      ],
      "metadata": {
        "id": "tgdnseHDB-5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "page = requests.get(\"https://www.tiktok.com/legal/page/us/privacy-policy/en\")\n",
        "\n",
        "with open(\"tiktok.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(page.text)"
      ],
      "metadata": {
        "id": "tXsAX2exCFS3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import BSHTMLLoader\n",
        "import re\n",
        "\n",
        "loader = BSHTMLLoader(\"tiktok.html\")\n",
        "data = loader.load()\n",
        "\n",
        "data[0].page_content = re.sub(\"\\n\\n+\", \"\\n\", data[0].page_content)"
      ],
      "metadata": {
        "id": "VrUzKfg6S7tE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Estimate the number of tokens of the text using the rough rule that a token is 3/4 of a word."
      ],
      "metadata": {
        "id": "gO2gX3MMVLjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Estimated tokens: {int(len(data[0].page_content) * 4 / 3)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3MnPUYwVWdS",
        "outputId": "4ba25488-1fe5-4c2e-8ac5-31666750234a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated tokens: 38822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of tokens of this text is about 40k, so it might be able to be summarized in a single copied and pasted prompt from a technology perspective, but realistically that would not make for a good user experience.\n",
        "\n",
        "To solve this problem we will use MapReduce to divide the text in multiple chunks and summarize each chunk individually. To do this, we could just use LangChain's load_summarize_chain utility. However, for the sake of visualization, we will implement each step in a simple and understandable way.\n",
        "\n",
        "First, we will divide the text in chunks using LangChain's RecursiveCharacterTextSplitter utility."
      ],
      "metadata": {
        "id": "ft9onP5BVnbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "chunk_size = 2000\n",
        "chunk_overlap = 100\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        ")\n",
        "\n",
        "splits = text_splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "_R5Dm0UhpvCe"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MECb6G4-rR4N",
        "outputId": "e503e368-8d04-436a-9dd3-0505cddfdb4a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "splits[0]"
      ],
      "metadata": {
        "id": "x5I3LTEW5gnP",
        "outputId": "b9026916-d13a-4bd2-c948-438b5875853c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Privacy Policy | TikTokU.S.Privacy PolicyLast updated: March 28, 2024This Privacy Policy applies to TikTok services (the â\\x80\\x9cPlatformâ\\x80\\x9d), which include TikTok apps, websites, software and related services accessed via any platform or device that link to this Privacy Policy. The Platform is provided and controlled by TikTok Inc. (â\\x80\\x9cTikTokâ\\x80\\x9d, â\\x80\\x9cweâ\\x80\\x9d or â\\x80\\x9cusâ\\x80\\x9d). We are committed to protecting and respecting your privacy. This Privacy Policy explains how we collect, use, share, and otherwise process the personal information of users and other individuals age 13 and over in connection with our Platform. For information about how we collect, use, share, and otherwise process the personal information of users under age 13 (â\\x80\\x9cChildrenâ\\x80\\x9d), please refer to our Childrenâ\\x80\\x99s Privacy Policy. For information about how we collect, use, share, and otherwise process consumer health data as defined under Washingtonâ\\x80\\x99s My Health My Data Act and other similar state laws, please refer to the Consumer Health Data Privacy Policy.Capitalized terms that are not defined in this Privacy Policy have the meaning given to them in theÂ\\xa0Terms of Service.What Information We CollectWe may collect information from and about you, including information that you provide, information from other sources, and automatically collected information.Information You ProvideWhen you create an account, upload content, contact us directly, or otherwise use the Platform, you may provide some or all of the following information:Account and profile information, such as name, age, username, password, language, email, phone number, social media account information, and profile image.User-generated content, including comments, photographs, livestreams, audio recordings, videos, text, hashtags, and virtual item videos that you choose to create with or upload to the Platform (â\\x80\\x9cUser Contentâ\\x80\\x9d) and the associated metadata, such as when, where, and by whom the content was created. Even if you are not a user,', metadata={'source': 'tiktok.html', 'title': 'Privacy Policy | TikTok'})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the text is divided, we do a summary of each chunk. We have the LLM make a summary in bullet point format for easy readability."
      ],
      "metadata": {
        "id": "Ew-cAUEozg4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "map_prompt = PromptTemplate(\n",
        "    template=\"\"\"Write a concise summary of the following text. The summary should be a list of bullet points. The summary cannot be more than 5 bullet points. The text is:\n",
        "{text}\n",
        "\n",
        "Summary: \"\"\",\n",
        "    input_variables=['text']\n",
        ")"
      ],
      "metadata": {
        "id": "xhPmvUHkpDxp"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = []\n",
        "\n",
        "for split in splits:\n",
        "  response = model.invoke(map_prompt.format(text=split.page_content))\n",
        "  summaries.append(response.content)"
      ],
      "metadata": {
        "id": "_958N3OgptId",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "926a1fcf-0be0-4ab3-98c5-30ff44fe1dd4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'Number of requests has exceeded your per-minute rate limit (https://docs.anthropic.com/claude/reference/rate-limits); see the response headers for current usage. Please try again later or contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-e41dc8253a39>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_prompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0msummaries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m         return cast(\n\u001b[1;32m    157\u001b[0m             \u001b[0mChatGeneration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    159\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    559\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     async def agenerate_prompt(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                     \u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_llm_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         flattened_outputs = [\n\u001b[1;32m    423\u001b[0m             \u001b[0mLLMResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerations\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_output\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 results.append(\n\u001b[0;32m--> 411\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    412\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 result = self._generate(\n\u001b[0m\u001b[1;32m    633\u001b[0m                     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_anthropic/chat_models.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/resources/messages.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     ) -> Message | Stream[MessageStreamEvent]:\n\u001b[0;32m--> 681\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0;34m\"/v1/messages\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1237\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m         )\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 921\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    922\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1005\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1053\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1002\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1005\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1053\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1018\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'type': 'error', 'error': {'type': 'rate_limit_error', 'message': 'Number of requests has exceeded your per-minute rate limit (https://docs.anthropic.com/claude/reference/rate-limits); see the response headers for current usage. Please try again later or contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:** What we are seeing here is a blocker on processing based on a rate limit error in Anthropic's Claude service. Since our experiment is to test this on the medium level plan at Anthropic, what we're seeing here is this approach is not completely viable. We are able to process some things so we will keep going and evaluate the usedfulness of the result."
      ],
      "metadata": {
        "id": "sY2Ll1dsqIBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will keep moving forward despite this Anthropic processing blocker."
      ],
      "metadata": {
        "id": "oZFXEfckraVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "qH9qN8-23kUg",
        "outputId": "615999dd-f849-43f5-dc7d-29d25e8d32ad"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here is a concise summary of the provided text in 5 bullet points:\\n\\n- This Privacy Policy applies to TikTok services, including apps, websites, software, and related services.\\n- TikTok collects various types of information from users, including account/profile information, user-generated content, and automatically collected information.\\n- Account/profile information includes name, age, username, password, email, phone number, and profile image.\\n- User-generated content includes comments, photos, livestreams, videos, text, hashtags, and virtual item videos uploaded by users.\\n- Automatically collected information includes device information, location data, usage data, and information from cookies and similar technologies.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8rF_w2aVrjy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have the summaries of at least some of the chunks. However, there are too many summaries to consolidate in a single prompt. So we will group them in groups that will fit in a single prompt. Then, we will consolidate the summaries in each group in a single summary. We will repeat this process until the number of groups is reduced to 1. This will mean that all the consolidated summaries now fit in a single summarization prompt."
      ],
      "metadata": {
        "id": "x7WebUtKz8in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def group_summaries(summaries, max_summaries):\n",
        "  groups = []\n",
        "  current_group = []\n",
        "  for summary in summaries:\n",
        "    current_group.append(summary)\n",
        "    if len(current_group) >= 10:\n",
        "      groups.append(current_group)\n",
        "      current_group = []\n",
        "  if len(current_group) > 0:\n",
        "    groups.append(current_group)\n",
        "  return groups\n",
        "\n",
        "groups = group_summaries(summaries, 10)"
      ],
      "metadata": {
        "id": "eeLBzSLjt1rA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(groups)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUty2leR1q82",
        "outputId": "5b544faf-b388-4d58-d962-a87e331664a7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combine_prompt = PromptTemplate(\n",
        "    template= \"\"\"The following is set of bullet-point summaries:\n",
        "{docs}\n",
        "Take these and distill it into a consolidated bullet-point summary of the main themes. Remove the bullet points that are not relevant to the whole text. The consolidated summary cannot be more than 7 bullet points.\n",
        "Helpful Answer: \"\"\",\n",
        "    input_variables=['docs']\n",
        ")"
      ],
      "metadata": {
        "id": "XzRy8IV6xpHH"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while len(groups) > 1:\n",
        "  new_summaries = []\n",
        "  for group in groups:\n",
        "    response = model.invoke(combine_prompt.format(docs=\"\\n\".join(group)))\n",
        "    new_summaries.append(response.content)\n",
        "  groups = group_summaries(new_summaries, 10)"
      ],
      "metadata": {
        "id": "ntyrecKPvdKu"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groups[0][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "SaW-VaJd36H7",
        "outputId": "1a4b6134-8c94-4fb9-e90f-4f9d7915301e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Here is a concise summary of the provided text in 5 bullet points:\\n\\n- This Privacy Policy applies to TikTok services, including apps, websites, software, and related services.\\n- TikTok collects various types of information from users, including account/profile information, user-generated content, and automatically collected information.\\n- Account/profile information includes name, age, username, password, email, phone number, and profile image.\\n- User-generated content includes comments, photos, livestreams, videos, text, hashtags, and virtual item videos uploaded by users.\\n- Automatically collected information includes device information, location data, usage data, and information from cookies and similar technologies.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, all summaries fit in a single prompt. All that's left to do is run a last prompt to do a final consolidation of all summaries into a single final summary. This summary will be in regular sentences, instead of bullet points, because that's what we want for this use case."
      ],
      "metadata": {
        "id": "PJ2bVWtl0Vti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_prompt = PromptTemplate(\n",
        "    template= \"\"\"The following is set of bullet-point summaries:\n",
        "{docs}\n",
        "Take these and distill it into a final, consolidated summary of the main themes. The final summary should be at most 15 sentences.\n",
        "Helpful Answer: \"\"\",\n",
        "    input_variables=['docs']\n",
        ")"
      ],
      "metadata": {
        "id": "A3qHDZtOpNTO"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(reduce_prompt.format(docs=\"\\n\".join(groups[0])))\n",
        "final_summary = response.content"
      ],
      "metadata": {
        "id": "38ixdtgdwuV9"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final result:** Below is the final summary of the TikTok privacy policy based on this MapReduce and LangChain approach."
      ],
      "metadata": {
        "id": "LLLY_6q-rzVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "6GJUpGKXw8lx",
        "outputId": "86d4e313-d532-4f60-e562-2b5f4a0774e4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Here is a consolidated 15-sentence summary of the main themes from the provided bullet points:\\n\\nTikTok's Privacy Policy outlines the various types of information collected from users of its services, including apps, websites, and related offerings. This includes account/profile information like name, age, username, password, email, phone number, and profile images. User-generated content such as comments, photos, videos, text, hashtags, and virtual items are also collected. \\n\\nAutomatically collected information encompasses device information, location data based on SIM cards and IP addresses, usage data, cookies, and identifiers assigned across devices to track user activity. User content is analyzed for objects, scenes, faces, text, and audio characteristics, and biometric data like faceprints and voiceprints may be collected with permission.\\n\\nOther data collected includes messages and metadata, clipboard information with permission, purchase and transaction histories, phone and social network contacts with permission, choices and communication preferences, verification information, survey/promotion participation details, information from third-party sign-in services, and data from advertisers and partners about off-platform activities. Information may also come from affiliated entities, public sources, authorities, organizations, and user feedback.\\n\\nThe range of data collected allows TikTok to provide its services while also facilitating advertising, analytics, product improvement, security, and compliance efforts. Users should review the full policy carefully to understand TikTok's data practices.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion:** What we see here is a more useful summary than what came from the previous approaches. It is fairly short but still contains relevant data. You can see what type of profile information is collected, what types of content are also collected. You can even see what is collected automatically from your phone including location data from your SIM card and your IP address. It also shows that it analyzes user content including face and audio, transaction histories, and biometric data with permission."
      ],
      "metadata": {
        "id": "B37dqOo3tl5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "The information in company privacy policies is important to understand. In the example of TikTok, where concern over the application's use of customer data has come to light, we examine if the methods used here can make that easier and more transparent for users of the app to understand this risks.\n",
        "\n",
        "We tried two approaches using the Anthropic LLM and LangChain capabilities:\n",
        "* First approach: A simple summarization based on manually copying and pasting a snippet of information from the privacy policy into the cell in this notebook. The result was a short but overly summarized version that was not very useful.\n",
        "* Second approach: A summarization using MapReduce and LangChain. While we did encounter some rate limiting that resulted in processing errors (keeping to a medium level Anthropic service), this approach did result in a final summary that was more useful than the first approach. We were able to gather from a single summary paragraph some impactful disclosures about using the TikTok application. These included automatic location and IP collection as well as face and audio among others.\n",
        "\n",
        "While more investigation should continue, this is an improved approach to helping end users more easily discover what the impacts to their personal information from using TikTok specifically. It also provides some examples to build on for additional investigation."
      ],
      "metadata": {
        "id": "NDFqYe3TwwBc"
      }
    }
  ]
}